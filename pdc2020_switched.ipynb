{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dapnn.imports import *\n",
    "from dapnn.data_processing import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducible results\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keeping only the traces in the log that will create the normal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path='data/csv/PDC2020_training/'\n",
    "log = import_log(log_path)\n",
    "\n",
    "num_traces = len(log['trace_id'].unique())\n",
    "ratio = 0.8\n",
    "traces_for_normal_model = int(num_traces * ratio)\n",
    "\n",
    "log = log[log['trace_id'].str.split(' ').str[1].astype(int) < traces_for_normal_model]\n",
    "log.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_dl(log,cat_names='activity',seed=45,ws=5,bs=32):\n",
    "    categorify=Categorify()\n",
    "    o=PPObj(log,procs=categorify,cat_names=cat_names,y_names=cat_names,splits=split_traces(log,test_seed=seed,validation_seed=seed))\n",
    "    dls=o.get_dls(windows=partial(windows_fast,ws=ws),bs=bs)\n",
    "    return o,dls,categorify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ControlFlowModel(torch.nn.Module) :\n",
    "    def __init__(self, o) :\n",
    "        super().__init__()\n",
    "        hidden=25\n",
    "        vocab_act=len(o.procs.categorify['activity'])\n",
    "        emb_dim_act = int(np.sqrt(vocab_act))+1\n",
    "\n",
    "        self.emb_act = nn.Embedding(vocab_act,emb_dim_act)\n",
    "        \n",
    "        self.lstm_act = nn.LSTM(emb_dim_act, hidden, batch_first=True, num_layers=2)\n",
    "\n",
    "        self.linear_act = nn.Linear(hidden, vocab_act)\n",
    "\n",
    "    def forward(self, xcat):\n",
    "        xcat=xcat[:,0]\n",
    "        x_act=xcat\n",
    "        x_act = self.emb_act(x_act)\n",
    "        x_act,_ = self.lstm_act(x_act)\n",
    "        x_act = x_act[:,-1]\n",
    "        x_act = self.linear_act(x_act)\n",
    "        return x_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HideOutput:\n",
    "    'A utility function that hides all outputs in a context'\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "\n",
    "\n",
    "def training_loop(learn,epoch,print_output,lr_find,fixed_learning_rate=0.01):\n",
    "    '''\n",
    "    Basic training loop that uses learning rate finder and one cycle training. \n",
    "    See fastai docs for more information\n",
    "    '''\n",
    "    if lr_find:\n",
    "        lr=np.median([learn.lr_find(show_plot=print_output)[0] for i in range(5)])\n",
    "        learn.fit_one_cycle(epoch,float(lr))\n",
    "    else: learn.fit(epoch,fixed_learning_rate)\n",
    "\n",
    "\n",
    "def train_validate(dls,m,metrics=accuracy,loss=F.cross_entropy,epoch=20,print_output=True,model_dir=\".\",lr_find=True,\n",
    "                   patience=5,min_delta=0.005,show_plot=True,store_path='tmp_switched',model_name='.model'):\n",
    "    '''\n",
    "    Trains a model on the training set with early stopping based on the validation loss.\n",
    "    Afterwards, applies it to the test set.\n",
    "    '''\n",
    "    cbs = [\n",
    "      EarlyStoppingCallback(monitor='valid_loss',min_delta=min_delta, patience=patience),\n",
    "      SaveModelCallback(fname=model_name),\n",
    "      ]\n",
    "    learn=Learner(dls, m, path=store_path, model_dir=model_dir, loss_func=loss ,metrics=metrics,cbs=cbs)\n",
    "\n",
    "    if print_output:\n",
    "        training_loop(learn,epoch,show_plot,lr_find=lr_find)\n",
    "        return learn.validate(dl=dls[2])\n",
    "    else:\n",
    "        with HideOutput(),learn.no_bar(),learn.no_logging():\n",
    "            training_loop(learn,epoch,show_plot,lr_find=lr_find)\n",
    "            return learn.validate(dl=dls[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeeze_cross_entropy = lambda x,y:F.cross_entropy(x,y[0])\n",
    "squeeze_accuracy =lambda x,y:accuracy(x,y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o,dls,categorify = training_dl(log)\n",
    "m = ControlFlowModel(o)\n",
    "train_val = train_validate(dls,m,epoch=20,metrics=squeeze_accuracy,loss=squeeze_cross_entropy, show_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = 'data/csv/PDC2020_ground_truth/'\n",
    "test_log = import_log(test_path)\n",
    "test_log[test_log['normal'] == False].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test(test_log,categorify,cat_names='activity'):\n",
    "    o=PPObj(test_log,procs=categorify,cat_names=cat_names,y_names=cat_names,do_setup=False)\n",
    "    o.process() # map to the same categories as in the training set\n",
    "    return o\n",
    "\n",
    "\n",
    "def predict_next_step(o,m,ws=5):\n",
    "    wds,idx=partial(windows_fast,ws=ws)(o.xs, o.event_ids)\n",
    "    res=(m(LongTensor(wds).cuda()))\n",
    "    return res,idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_if_anomaly(res,o,idx,k=6):\n",
    "    # for each activity, it is an anomaly if its not in the top k most likely activities in the next step\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    y = o.items['activity'].iloc[idx].values\n",
    "    p = sm(res)\n",
    "    if_anomaly = []\n",
    "    for i in range(len(y)):\n",
    "        if_anomaly.append(1 if y[i] not in p[i].argsort(descending=True)[:k] else 0)\n",
    "    return np.array(if_anomaly)\n",
    "\n",
    "\n",
    "def get_anomalies_if_anomaly(if_anomaly,o,idx,r=0.01):\n",
    "    \"\"\"If a trace has more than r of its activities as anomalies, it is an anomaly\"\"\"\n",
    "    df = pd.DataFrame(columns=['if_anomaly'])\n",
    "    df['if_anomaly'] = if_anomaly\n",
    "    df['trace_id'] = o.items.iloc[idx]['trace_id'].values\n",
    "    df['normal'] = o.items.iloc[idx]['normal'].values\n",
    "    y_true = (df.loc[df.trace_id.drop_duplicates().index].normal==False).tolist()\n",
    "    cases = df.loc[df.trace_id.drop_duplicates().index].trace_id.tolist()\n",
    "    # for each trace, if more than r of its activities are anomalies, it is an anomaly\n",
    "\n",
    "    anomalies = set(list(df.loc[df.groupby('trace_id')['if_anomaly'].transform('mean') > r]['trace_id']))\n",
    "    y_pred=[case in anomalies for case in cases]\n",
    "    return y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = process_test(test_log,categorify)\n",
    "nsp, idx = predict_next_step(o, m)\n",
    "\n",
    "if_anomaly = calc_if_anomaly(nsp, o, idx)\n",
    "y_pred, y_true = get_anomalies_if_anomaly(if_anomaly,o,idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'F1 score: {f1_score(y_true, y_pred)}')\n",
    "print(f'Accuracy score: {accuracy_score(y_true, y_pred)}')\n",
    "print(f'Precision score: {precision_score(y_true, y_pred)}')\n",
    "print(f'Recall score: {recall_score(y_true, y_pred)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dapnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
